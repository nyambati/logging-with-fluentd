# Default values for fluentd.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: nginx
  tag: stable
  pullPolicy: IfNotPresent

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths: []

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

fluentd:
  kubernetes:
    sources: |-
      <source>
        @type tail
        path /var/log/containers/*.log
        pos_file /var/log/fluentd-containers.log.pos
        tag kubernetes.*
        read_from_head true
        time_format %Y-%m-%dT%H:%M:%S.%NZ
        format json
      </source>
    ignore:
      - fluentd
      - kube-system
      - dashboard
      - default
      - filebeat
      - monitoring
    filters: |-
      <filter kubernetes.**>
        @type parser
        key_name log
        <parse>
          @type multi_format
          <pattern>
            format json
          </pattern>
          <pattern>
            format nginx
          </pattern>
          <pattern>
            format none
          </pattern>
        </parse>
      </filter>

      <filter kubernetes.**>
        @type kubernetes_metadata
        cache_size 1000
      </filter>
      <filter kubernetes.**>
        @type record_transformer
        enable_ruby
        # cater for loggers with msg as message key
        <record>
          message ${ record["msg"] ? record["msg"] : record["message"]}
        </record>
        remove_keys msg
      </filter>
    removeKeys:
      - match: kubernetes.**
        keys: 
          - $["kubernetes"]["pod_name"]
          - $["kubernetes"]["container_image_id"]
          - $["docker"]["container_id"]
          - $["kubernetes"]["namespace_id"]
          - $["kubernetes"]["pod_id"]
          - $["kubernetes"]["namespace_labels"]
          - $["kubernetes"]["master_url"]
          - $["kubernetes"]["labels"]["pod-template-hash"]
          - $["kubernetes"]["labels"]["app_kubernetes_io/version"]
          - $["kubernetes"]["labels"]["heritage"] 
          - $["kubernetes"]["labels"]["app_kubernetes_io/version"]
          - $["kubernetes"]["labels"]["app_kubernetes_io/managed-by"]
          - $["kubernetes"]["labels"]["helm_sh/chart"]
          - $["v"]
  outputs: |-
    <match **>
      @type elasticsearch_dynamic
      @id out_es
      @log_level info
      include_tag_key true
      host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
      port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
      scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME']}"
      password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
      password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
      reload_connections "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'false'}"
      reconnect_on_error "#{ENV['FLUENT_ELASTICSEARCH_RECONNECT_ON_ERROR'] || 'true'}"
      reload_on_failure "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_ON_FAILURE'] || 'true'}"
      log_es_400_reason "#{ENV['FLUENT_ELASTICSEARCH_LOG_ES_400_REASON'] || 'false'}"
      logstash_prefix fluentd-${record["kubernetes"]["namespace_name"]}
      logstash_format "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_FORMAT'] || 'true'}"
      index_name "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_INDEX_NAME'] || 'fluentd'}"
      type_name "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_TYPE_NAME'] || 'fluentd'}"
      rollover_index true
      deflector_alias "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_INDEX_NAME'] || 'fluentd'}"
      <buffer>
        flush_thread_count "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_THREAD_COUNT'] || '8'}"
        flush_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL'] || '5s'}"
        queue_limit_length "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH'] || '32'}"
        retry_max_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_RETRY_MAX_INTERVAL'] || '30'}"
        retry_forever true
      </buffer>
    </match>


env:
  FLUENT_ELASTICSEARCH_SED_DISABLE: "true"
  FLUENT_ELASTICSEARCH_HOST: elasticsearch.default
  FLUENT_ELASTICSEARCH_PORT: 9200
  FLUENT_ELASTICSEARCH_SCHEME: "http"
  FLUENT_ELASTICSEARCH_LOG_ES_400_REASON: true